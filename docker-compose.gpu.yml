services:
  tts-gpu-0:
    image: ${GPU_IMAGE:-qwen3-tts-gpu}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE: ${GPU_BASE_IMAGE:-nvcr.io/nvidia/pytorch:23.10-py3}
        FEATURES: ${GPU_FEATURES:-flash-attn,cli,server}
        CUDA_COMPUTE_CAP: ${CUDA_COMPUTE_CAP:-86}
    environment:
      BASE_MODEL_DIR: ${BASE_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-Base}
      VOICE_DESIGN_MODEL_DIR: ${VOICE_DESIGN_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-VoiceDesign}
      TOKENIZER_DIR: ${TOKENIZER_DIR:-/models/tokenizer}
      DEVICE: cuda
      MAX_CONCURRENCY: ${MAX_CONCURRENCY:-1}
      NVIDIA_VISIBLE_DEVICES: "0"
    volumes:
      - ${MODEL_ROOT:-./models}:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    restart: unless-stopped

#  tts-gpu-1:
#    image: ${GPU_IMAGE:-qwen3-tts-gpu}
#    build:
#      context: .
#      dockerfile: Dockerfile
#      args:
#        BASE: ${GPU_BASE_IMAGE:-nvcr.io/nvidia/pytorch:23.10-py3}
#        FEATURES: ${GPU_FEATURES:-flash-attn,cli,server}
#        CUDA_COMPUTE_CAP: ${CUDA_COMPUTE_CAP:-86}
#    environment:
#      BASE_MODEL_DIR: ${BASE_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-Base}
#      VOICE_DESIGN_MODEL_DIR: ${VOICE_DESIGN_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-VoiceDesign}
#      TOKENIZER_DIR: ${TOKENIZER_DIR:-/models/tokenizer}
#      DEVICE: cuda
#      MAX_CONCURRENCY: ${MAX_CONCURRENCY:-1}
#      NVIDIA_VISIBLE_DEVICES: "1"
#    volumes:
#      - ${MODEL_ROOT:-./models}:/models:ro
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              device_ids: ["1"]
#              capabilities: [gpu]
#    restart: unless-stopped

  tts-gpu-2:
    image: ${GPU_IMAGE:-qwen3-tts-gpu}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE: ${GPU_BASE_IMAGE:-nvcr.io/nvidia/pytorch:23.10-py3}
        FEATURES: ${GPU_FEATURES:-flash-attn,cli,server}
        CUDA_COMPUTE_CAP: ${CUDA_COMPUTE_CAP:-86}
    environment:
      BASE_MODEL_DIR: ${BASE_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-Base}
      VOICE_DESIGN_MODEL_DIR: ${VOICE_DESIGN_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-VoiceDesign}
      TOKENIZER_DIR: ${TOKENIZER_DIR:-/models/tokenizer}
      DEVICE: cuda
      MAX_CONCURRENCY: ${MAX_CONCURRENCY:-1}
      NVIDIA_VISIBLE_DEVICES: "2"
    volumes:
      - ${MODEL_ROOT:-./models}:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["2"]
              capabilities: [gpu]
    restart: unless-stopped

#  tts-gpu-3:
#    image: ${GPU_IMAGE:-qwen3-tts-gpu}
#    build:
#      context: .
#      dockerfile: Dockerfile
#      args:
#        BASE: ${GPU_BASE_IMAGE:-nvcr.io/nvidia/pytorch:23.10-py3}
#        FEATURES: ${GPU_FEATURES:-flash-attn,cli,server}
#        CUDA_COMPUTE_CAP: ${CUDA_COMPUTE_CAP:-86}
#    environment:
#      BASE_MODEL_DIR: ${BASE_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-Base}
#      VOICE_DESIGN_MODEL_DIR: ${VOICE_DESIGN_MODEL_DIR:-/models/Qwen3-TTS-12Hz-1.7B-VoiceDesign}
#      TOKENIZER_DIR: ${TOKENIZER_DIR:-/models/tokenizer}
#      DEVICE: cuda
#      MAX_CONCURRENCY: ${MAX_CONCURRENCY:-1}
#      NVIDIA_VISIBLE_DEVICES: "3"
#    volumes:
#      - ${MODEL_ROOT:-./models}:/models:ro
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              device_ids: ["3"]
#              capabilities: [gpu]
#    restart: unless-stopped

  nginx:
    image: nginx:1.27-alpine
    depends_on:
      - tts-gpu-0
      - tts-gpu-2
#      - tts-gpu-1
#      - tts-gpu-3
    ports:
      - ${HTTP_PORT:-19160}:8000
    volumes:
      - ./nginx-gpu.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
